Hereâ€™s a **professional README.md** tailored for your project (â€œAI Interview Feedbackâ€) that explains what it does, how to run it locally, and how to deploy on **Google Cloud Run**. Itâ€™s written so someone picking up the repo can get going fast.

---

```markdown
# ğŸ¤ AI Interview Feedback

An end-to-end system that analyzes interview answers from **audio** (and optional **video**) to give instant feedback on:

- **Speech-to-Text** transcription (Whisper)
- **Sentiment Analysis** (DistilBERT SST-2)
- **Clarity** (filler word detection)
- **Pace** (words per minute scoring)
- **Confidence** (facial emotion heuristic from video)
- **Overall Score** (combined)

Frontend: a simple HTML dashboard  
Backend: Flask API (Python 3.11)  
Deployment: Docker + Cloud Run (GCP)

---

## ğŸš€ Features

- Upload an **audio file** (`.wav`, `.mp3`)  
- (Optional) Upload a **video file** (`.mp4`, `.avi`, etc.)  
- See **transcript, sentiment, clarity, pace, confidence, and overall score** in real time  
- Containerized with **Docker** for reproducibility  
- Runs on **Cloud Run** with CPU-only Torch + ffmpeg  

---

## ğŸ“‚ Project Structure

```

ai-interview-feedback/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app.py                # Flask API
â”‚   â”œâ”€â”€ speech\_analysis.py    # Whisper transcription
â”‚   â”œâ”€â”€ sentiment\_analysis.py # DistilBERT sentiment
â”‚   â”œâ”€â”€ feedback.py           # Clarity & pace scoring
â”‚   â”œâ”€â”€ facial\_analysis.py    # Facial emotion heuristic
â”‚   â”œâ”€â”€ templates/
â”‚   â”‚   â””â”€â”€ index.html        # Frontend dashboard
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ Dockerfile
â””â”€â”€ README.md

````

---

## âš™ï¸ Setup (Local)

### 1. Clone & install
```bash
git clone https://github.com/your-username/ai-interview-feedback.git
cd ai-interview-feedback/backend

python -m venv venv
source venv/bin/activate    # or venv\Scripts\activate on Windows

pip install -r requirements.txt
````

### 2. Run locally

```bash
python app.py
```

Visit â†’ [http://127.0.0.1:8080](http://127.0.0.1:8080)

---

## ğŸ³ Run with Docker (Local)

```bash
# build
docker build -t interview-feedback .

# run
docker run -p 8080:8080 interview-feedback
```

---

## â˜ï¸ Deploy on Google Cloud Run

1. **Enable APIs**

```bash
gcloud services enable run.googleapis.com cloudbuild.googleapis.com artifactregistry.googleapis.com
```

2. **Build & push container**

```bash
gcloud builds submit --tag gcr.io/PROJECT_ID/interview-feedback
```

3. **Deploy**

```bash
gcloud run deploy interview-feedback \
  --image gcr.io/PROJECT_ID/interview-feedback \
  --platform managed \
  --region us-central1 \
  --allow-unauthenticated \
  --memory=4Gi \
  --cpu=2 \
  --timeout=900 \
  --concurrency=1
```

4. **Get URL**

```
Service [interview-feedback] revision deployed to [https://your-service-url.run.app]
```

---

## ğŸ”§ Configuration

* `WHISPER_SIZE`: `tiny | base | small` (default: `base`)
* `UPLOAD_DIR`: override upload folder (default: `/tmp/uploads` for Cloud Run)
* `DEFAULT_CONFIDENCE`: default confidence score if no video (default: `0.5`)

Set via environment variables locally or in Cloud Run.

---

## ğŸ“Š Example Output

```json
{
  "transcription": "Thank you for the opportunity...",
  "sentiment": { "label": "POSITIVE", "score": 0.87 },
  "clarity": 0.92,
  "pace": 0.88,
  "confidence": 0.7,
  "overall": 0.84
}
```

---

## ğŸ“ Notes & Limitations

* Whisper is CPU-only by default (Cloud Run doesnâ€™t provide GPUs).
* Facial analysis currently uses **ResNet18 stub**, not a real FER-trained model â†’ heuristic only.
* Best for English speech, short clips (<30 min).
* Cold starts may take \~10s while models load; can prewarm during build.

---

## ğŸ“Œ Roadmap

* âœ… CPU-only deployment on Cloud Run
* â³ Replace ResNet18 with a real **FER model** (e.g. RAF-DB)
* â³ Add **real-time streaming** (WebSocket + faster-whisper)
* â³ Store & compare sessions in **GCS/Firestore**
* â³ Build dashboard with **React**

---

## ğŸ§‘â€ğŸ’» Authors
Jatin Bagga
Chhavi Tokkhi
